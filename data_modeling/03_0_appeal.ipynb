{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "seed_value= 0\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "ipython = get_ipython()\n",
    "\n",
    "def hide_traceback(exc_tuple=None, filename=None, tb_offset=None,\n",
    "                      exception_only=False, running_compiled_code=False):\n",
    "       etype, value, tb = sys.exc_info()\n",
    "       return ipython._showtraceback(etype, value, ipython.InteractiveTB.get_exception_only(etype, value))\n",
    "\n",
    "ipython.showtraceback = hide_traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = os.path.join(os.path.dirname(os.getcwd()), 'xlsx_data', f\"{'xlsx_#10_final'}.xlsx\")\n",
    "cname = os.path.join(os.path.dirname(os.getcwd()), 'xlsx_data', f\"{'xlsx_#10_case'}.xlsx\")\n",
    "lname = os.path.join(os.path.dirname(os.getcwd()), 'xlsx_data', f\"{'xlsx_#10_law'}.xlsx\")\n",
    "\n",
    "dff = pd.read_excel(fname, index_col=0)\n",
    "dfc = pd.read_excel(cname, index_col=0)\n",
    "dfl = pd.read_excel(lname, index_col=0)\n",
    "\n",
    "df = pd.concat([dff[['text', 'y_appeal']], dfc[1:], dfl[1:]], axis=1).reindex(dff.index)\n",
    "df = df.dropna(subset=['y_appeal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = df[df['y_appeal'] == 0].sample(frac=1)\n",
    "df_1 = df[df['y_appeal'] == 1].sample(frac=1)\n",
    "\n",
    "sample_size = len(df_0) if len(df_0) < len(df_1) else len(df_1)\n",
    "\n",
    "df = pd.concat([df_0.head(sample_size), df_1.head(sample_size)]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('y_appeal', axis=1)\n",
    "y = tf.keras.utils.to_categorical(df['y_appeal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train = X_train[dfc.columns[1:]]\n",
    "X1_test = X_test[dfc.columns[1:]]\n",
    "\n",
    "X2_train = X_train[dfl.columns[1:]]\n",
    "X2_test = X_test[dfl.columns[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3_train = list(X_train[\"text\"])\n",
    "X3_test = list(X_test[\"text\"])\n",
    "\n",
    "max_features = 30000\n",
    "sequence_length = 256\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_features, split=' ', oov_token='<unw>')\n",
    "tokenizer.fit_on_texts(X3_train)\n",
    "\n",
    "X3_train = tokenizer.texts_to_sequences(X3_train)\n",
    "X3_test = tokenizer.texts_to_sequences(X3_test)\n",
    "\n",
    "X3_train = tf.keras.preprocessing.sequence.pad_sequences(X3_train, sequence_length)\n",
    "X3_test = tf.keras.preprocessing.sequence.pad_sequences(X3_test, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = tf.keras.Input(dtype = tf.float32, shape = (len(dfc.columns[1:]),))\n",
    "input_2 = tf.keras.Input(dtype = tf.float32, shape = (len(dfl.columns[1:]),))\n",
    "\n",
    "dense_layer_1_1 = tf.keras.layers.Dense(units = 10, activation = tf.nn.relu)(input_1)\n",
    "dense_layer_1_2 = tf.keras.layers.Dense(units = 10, activation = tf.nn.relu)(dense_layer_1_1)\n",
    "dense_layer_1_3 = tf.keras.layers.Dense(units = 10, activation = tf.nn.relu)(dense_layer_1_2)\n",
    "dense_layer_1_4 = tf.keras.layers.Dense(units = 10, activation = tf.nn.relu)(dense_layer_1_3)\n",
    "dropout_1_5 = tf.keras.layers.Dropout(rate = 0.5)(dense_layer_1_4)\n",
    "\n",
    "\n",
    "dense_layer_2_1 = tf.keras.layers.Dense(units = 10, activation = tf.nn.relu)(input_2)\n",
    "dense_layer_2_2 = tf.keras.layers.Dense(units = 10, activation = tf.nn.relu)(dense_layer_2_1)\n",
    "dense_layer_2_3 = tf.keras.layers.Dense(units = 10, activation = tf.nn.relu)(dense_layer_2_2)\n",
    "dense_layer_2_4 = tf.keras.layers.Dense(units = 10, activation = tf.nn.relu)(dense_layer_2_3)\n",
    "dropout_2_5 = tf.keras.layers.Dropout(rate = 0.5)(dense_layer_2_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join(os.path.dirname(os.getcwd()), 'data_processing', 'ft_0825_5.txt'),  encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "num_words = min(max_features, len(word_index)) + 1\n",
    "print(num_words)\n",
    "embedding_dim = 200\n",
    "num_filters = 100\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i > max_features:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        embedding_matrix[i] = np.random.randn(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_3 = tf.keras.Input(dtype = tf.float32, shape = (sequence_length,))\n",
    "embedding_layer_3 = tf.keras.layers.Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "                            input_length=sequence_length,\n",
    "                            trainable=True)(input_3)\n",
    "\n",
    "reshape_3 = tf.keras.layers.Reshape((sequence_length, embedding_dim, 1))(embedding_layer_3)\n",
    "\n",
    "conv_0_3 = tf.keras.layers.Conv2D(num_filters, kernel_size=(3, embedding_dim), activation=tf.nn.relu, kernel_regularizer=tf.keras.regularizers.l2(3))(reshape_3)\n",
    "conv_1_3 = tf.keras.layers.Conv2D(num_filters, kernel_size=(4, embedding_dim), activation=tf.nn.relu, kernel_regularizer=tf.keras.regularizers.l2(3))(reshape_3)\n",
    "conv_2_3 = tf.keras.layers.Conv2D(num_filters, kernel_size=(5, embedding_dim), activation=tf.nn.relu, kernel_regularizer=tf.keras.regularizers.l2(3))(reshape_3)\n",
    "\n",
    "maxpool_0_3 = tf.keras.layers.MaxPool2D(pool_size=(sequence_length - 3 + 1, 1), strides=(1,1), padding='valid')(conv_0_3)\n",
    "maxpool_1_3 = tf.keras.layers.MaxPool2D(pool_size=(sequence_length - 4 + 1, 1), strides=(1,1), padding='valid')(conv_1_3)\n",
    "maxpool_2_3 = tf.keras.layers.MaxPool2D(pool_size=(sequence_length - 5 + 1, 1), strides=(1,1), padding='valid')(conv_2_3)\n",
    "\n",
    "concatenated_tensor_3 = tf.keras.layers.Concatenate(axis=1)([maxpool_0_3, maxpool_1_3, maxpool_2_3])\n",
    "flatten_3 = tf.keras.layers.Flatten()(concatenated_tensor_3)\n",
    "dropout_3 = tf.keras.layers.Dropout(rate = 0.5)(flatten_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_layer = tf.keras.layers.Concatenate()([dropout_1_5, dropout_2_5, dropout_3])\n",
    "\n",
    "dense_layer_3 = tf.keras.layers.Dense(units = 10, activation = tf.nn.relu)(concat_layer)\n",
    "dense_layer_4 = tf.keras.layers.Dense(units = 10, activation = tf.nn.relu)(dense_layer_3)\n",
    "dense_layer_5 = tf.keras.layers.Dense(units = 10, activation = tf.nn.relu)(dense_layer_4)\n",
    "dense_layer_6 = tf.keras.layers.Dense(units = 10, activation = tf.nn.relu)(dense_layer_5)\n",
    "\n",
    "\n",
    "output = tf.keras.layers.Dense(units = 2, activation = tf.nn.softmax)(dense_layer_4)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_1, input_2, input_3], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=tf.keras.optimizers.SGD(learning_rate=0.0001), metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x=[X1_train, X2_train, X3_train], y=y_train, batch_size=2, epochs=100, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x=[X1_test, X2_test, X3_test], y=y_test, verbose=1)\n",
    "\n",
    "print(\"Test Score:\", score[0])\n",
    "print(\"Test ACC:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf-idf 0.2\n",
    "# 50-10-10-10-10 배치 2, 에폭 100, 0.01: 93 91 92// 60 55 52 55 53 52\n",
    "# 50-10-10-10-10 배치 2, 에폭 100, 0.001: 73 73 76 72 72\n",
    "# 50-10-10-10-10 배치 2, 에폭 100, 0.005: 90 65 91 87 52// 59 55 91 51 87 //48 50 94 81 56 // 82 94 53 54 58 => 70\n",
    "# 50-10-10-10-10 배치 2, 에폭 100, 0.002: 60 90 93 59 63// 89 72 57 59 89 // 87 67 59 70 85 // 57 60 68 90 67 => 72\n",
    "\n",
    "# 50-10-10-10-10 배치 2, 에폭 100, 0.0005: 74 91 88 76 88 // 75 77 90 92 71 // 76 90 93 79 76// 73 72 76 76 77 => 80\n",
    "# 50-10-10-10-10 배치 4, 에폭 100, 0.0005: 79 70 74 78 76 // 82 72 84 78 77 // 85 75 77 84 77\n",
    "# 50-10-10-10-10 배치 8, 에폭 100, 0.0005: 78 79 72 78 75 // 75 73 81 81 65\n",
    "# 50-10-10-10-10 배치 16, 에폭 100, 0.0005: 81 75 76 69 77 // \n",
    "\n",
    "# 100-100-100-100-100 배치 2, 에폭 100, 0.0005: 69 95 93 76 79 // 73 94 79 75 77 // 78 70 75 60 88 // 69 92 88 86 74 => 79\n",
    "# 10-10-10-10-10 배치 2, 에폭 100, 0.0005: 75 82 75 82 75 // 73 74 65 75 85\n",
    "# 100-10-10-10-10 배치 2, 에폭 100, 0.0005: 86 85 80 72 78 // 96 84 73 65 79 // => 80\n",
    "\n",
    "# 50-10-10-10-10 배치 2, 에폭 200, 0.0005: 72\n",
    "# 50-10-10-10-10 배치 2, 에폭 100, 0.0005, random: 89 89 88 89 87 //89 88 88 89 89 \n",
    "# 50-10-10-10-10 배치 2, 에폭 150, 0.0005, random: 87 87 87\n",
    "# 50-10-10-10-10 배치 2, 에폭 200, 0.0005, random: 86 85 \n",
    "\n",
    "# 50-10-10-10-10 배치 2, 에폭 100, 0.0005, random, w2v: 88 88 88\n",
    "\n",
    "# 50-10-10-10 배치 2, 에폭 100, 0.0005, random, 판사없이: 90 88 89 89 89 // 88 89 \n",
    "# 100-100-100-100 배치 2, 에폭 100, 0.0005, random, 판사없이: 85 \n",
    "# 100-10-10-10 배치 2, 에폭 100, 0.0005, random, 판사없이: 88\n",
    "# 8-8-8-8 배치 2, 에폭 100, 0.0005, random, 판사없이: 88\n",
    "# 10-10-10-10 배치 2, 에폭 100, 0.0005, random, 판사없이: 87\n",
    "\n",
    "\n",
    "\n",
    "# 50-10-10-10-10 배치 2, 에폭 100, 0.0001: 80 72 81 72 77 // 77 75 80 85 68 // 75\n",
    "# 50-10-10-10-10 배치 2, 에폭 100, 0.00005: 85 80 75 77 73// 73 \n",
    "\n",
    "# 50-10-10-10-10 배치 2, 에폭 100, 0.02: 51 50 53 54 45 56 54\n",
    "\n",
    "#tf-idf 0.1\n",
    "#50-10-10-10 배치 2, 에폭 100, 0.0005, random, 판사없이: 82\n",
    "\n",
    "\n",
    "\n",
    "# 50-10-10-10/10-10 배치 2, 에폭 100, 0.0005, random, 판사없이: 87 87\n",
    "# 50-10-10-10/10-10-10 배치 2, 에폭 100, 0.0005, random, 판사없이: 90 90 89 90 89 90 89 \n",
    "# 50-10-10-10/10-10-10-10 배치 2, 에폭 100, 0.0005, random, 판사없이: 88 89\n",
    "\n",
    "#new learning_rate=0.0005 에폭100: 72 // 에폭150: 75\n",
    "#lr 0.001 에폭100: 78 75 \n",
    "#lr 0.0001 74 \n",
    "#lr 0.005 54 \n",
    "#lr 0.002 71 \n",
    "\"\"\"\n",
    "#lr 0.001 에폭100: 78 75 78\n",
    "#lr 0.0005 배치2 에폭100: 77\n",
    "#lr 0.0005 배치32 에폭100: 74\n",
    "#lr 0.0005 배치16 에폭100: 74 \n",
    "#lr 0.0005 배치8 에폭100: 68\n",
    "#lr 0.0005 배치4 에폭100:74\n",
    "\n",
    "#lr 0.0005 배치2 10-10-10-(10) 에폭100: 78 \n",
    "#lr 0.0005 배치2 100-100-100-(100) 에폭100:73\n",
    "#lr 0.0005 배치2 200-200-200-(200) 에폭100: 73 \n",
    "#lr 0.0005 배치2 10-10-10-(10) cv drop 0.2 에폭100:73\n",
    "#lr 0.0005 배치2 10-(10-10-10) 에폭100: 75 \n",
    "#lr 0.0005 배치2 10-10-10-(10) 전체 drop 0.5 에폭100: 80 76 77\n",
    "#lr 0.0005 배치2 10-10-10-(10) 전체 drop 0.5 , l2=0.01 에폭100: 74\n",
    "#lr 0.0005 배치2 10-10-10-(10) 전체 drop 0.5 , l2=0.1 에폭100: 75\n",
    "#lr 0.0005 배치2 10-10-10-(10) 전체 drop 0.5 , l2=1 에폭100: 79 <=====loss 더 적은 걸로\n",
    "#lr 0.0005 배치2 10-10-10-(10) 전체 drop 0.5 , l2=2 에폭100: 76\n",
    "#lr 0.0005 배치2 10-10-10-(10) 전체 drop 0.5 , l2=4 에폭100: 80 <=====loss 더 적은 걸로\n",
    "#lr 0.0005 배치2 10-10-10-(10) 전체 drop 0.5 , l2=3 , sequence 128 에폭100: 80 ????????????????????????? 더 해보기\n",
    "#lr 0.0005 배치2 10*8-10*4-10*3 전체 drop 0.5, l2=3 에폭100: 74\n",
    "#lr 0.0005 배치2 128*8-10*4-10*3 전체 drop 0.5, l2=3 에폭100: 76\n",
    "#lr 0.0005 배치2 10*4-128*8-10*3 전체 drop 0.5, l2=3 에폭100: 76\n",
    "#lr 0.0005 배치2 10*4-10*4-128*4 전체 drop 0.5, l2=3 에폭100: 74 \n",
    "\n",
    "#lr 0.0005 배치2 10-10-10-(10) drop225 case 0.8 에폭100: 75 \n",
    "#lr 0.0005 배치2 10*1-10*4-10*3 전체 drop 0.5 case 0.8 에폭100: 79 ?????????/ 절박하면 층 줄이기\n",
    "#lr 0.0005 배치2 10*4-10*4-10*3 전체 drop 0.5 case 0.5 binary 에폭100: 75 \n",
    "#lr 0.0005 배치2 10*1-10*1-10*1 전체 drop 0.5 case 0.5 에폭100: 78 \n",
    "#lr 0.0005 배치2 10*1-10*1-10*1 max 1 전체 drop 0.5 case 0.5 에폭100: 74 \n",
    "#lr 0.0002 배치2 10*4-10*4-10*4 drop225 case 0.5 에폭100: 70\n",
    "#lr 0.0004 배치2 10*4-10*4-10*4 drop225 case 0.5 에폭100: 75\n",
    "#lr 0.0006 배치2 10*4-10*4-10*4 drop225 case 0.5 에폭100: 71\n",
    "#lr 0.0005 배치2 10*2-10*2-10*2 drop225 case 0.5 에폭100: 77\n",
    "#lr 0.0005 배치2 10*16-10*16-10*2 drop225 case 0.5 에폭100:75 \n",
    "#lr 0.0005 배치2 512*16-512*16-10*2 drop225 case 0.5 에폭100:73 \n",
    "#lr 0.0005 배치2 512*16-512*16-512*16 drop225 case 0.5 에폭100:79\n",
    "\n",
    "#lr 0.0005 배치2 1024*16-1024*16-1024*16 drop225 case 0.5 에폭100: 74----------------------------------\n",
    "\n",
    "\n",
    "#lr 0.0005 배치2 256*16-256*16-10*2 drop225 case 0.5 에폭100: 78  \n",
    "#lr 0.0005 배치2 256*16-256*16-256*2 drop225 case 0.5 에폭100:79  \n",
    "#lr 0.0005 배치2 256*16-256*16-256*16 drop225 case 0.5 에폭100: 78\n",
    "\n",
    "#lr 0.0005 배치2 128*16-128*16-10*2 drop225 case 0.5 에폭100: 75 \n",
    "#lr 0.0005 배치2 128*16-128*16-128*2 drop225 case 0.5 에폭100: 79 \n",
    "#lr 0.0005 배치2 128*16-128*16-128*16 drop225 case 0.5 에폭100: 75\n",
    "\n",
    "#lr 0.0005 배치2 64*16-64*16-64*2 drop225 case 0.5 에폭100: 81 78 76 //에폭150: 76\n",
    "#lr 0.0005 배치2 64*16-64*16-64*16 drop225 case 0.5 에폭100: 74\n",
    "\n",
    "#lr 0.0005 배치2 100*16-100*16-100*2 drop225 case 0.5 에폭100: 77 78\n",
    "#lr 0.0005 배치2 100*16-100*16-100*16 drop225 case 0.5 에폭100: 75\n",
    "\n",
    "#lr 0.0005 배치2 32*16-32*16-32*2 drop225 case 0.5 에폭100: 75\n",
    "#lr 0.0005 배치2 32*16-32*16-32*16 drop225 case 0.5 에폭100: 76\n",
    "\n",
    "#lr 0.0005 배치2 10*16-10*16-10*16 drop225 case 0.5 에폭100: 학습안됨\n",
    "\n",
    "#lr 0.0005 배치2 10-10-10-(10) 전체 drop 0.5 , l2=4 , sequence 128 에폭100:\n",
    "#lr 0.0005 배치2 10-10-10-(10) 전체 drop 0.5 , l2=1 , sequence 128 에폭100:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#adam lr 0.001 배치2 10-10-10-(10) 전체 drop 0.5 에폭100: 학습안됨\n",
    "#adam lr 0.0001 배치2 10-10-10-(10) 전체 drop 0.5 에폭100: 똑같음\n",
    "#Adadelta lr 0.0001 배치2 10-10-10-(10) 전체 drop 0.5 에폭100: 학습안됨\n",
    "#Adadelta lr 0.01 배치2 10-10-10-(10) 전체 drop 0.5 에폭100: 75\n",
    "#Adamax lr 0.001 배치2 10-10-10-(10) 전체 drop 0.5 에폭100: 78 // loss 굿\n",
    "#Adamax lr 0.0005 배치2 10-10-10-(10) 전체 drop 0.5 에폭100: 79\n",
    "#Adamax lr 0.005 배치2 10-10-10-(10) 전체 drop 0.5 에폭100: \n",
    "\n",
    "\n",
    "#Nadam lr 0.001 배치2 10-10-10-(10) drop225 에폭100: \n",
    "#RMSprop lr 0.001 배치2 10-10-10-(10) 전체 drop 0.5 에폭100: \n",
    "\n",
    "\n",
    "#lr 0.0005 배치2 10*1-10*4-10*3 전체 drop 0.5 에폭100:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#lr 0.0005 배치2 10-10-10-(10) 전체 drop 0.5 에폭1000: 79\n",
    "#lr 0.0005 배치2 10-10-10-(10) drop 0.5 0.2 0.5 에폭100: 76\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Adadelta 0.001 에폭100:학습안됨\n",
    "Adagrad 0.001 에폭100: 72\n",
    "adam 0.001 에폭100: \n",
    "Adamax 0.001 에폭100:\n",
    "Nadam 0.001 에폭100:\n",
    "RMSprop 0.001 에폭100:\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
