{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "seed_value= 0\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "ipython = get_ipython()\n",
    "\n",
    "def hide_traceback(exc_tuple=None, filename=None, tb_offset=None,\n",
    "                      exception_only=False, running_compiled_code=False):\n",
    "       etype, value, tb = sys.exc_info()\n",
    "       return ipython._showtraceback(etype, value, ipython.InteractiveTB.get_exception_only(etype, value))\n",
    "\n",
    "ipython.showtraceback = hide_traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = os.path.join(os.path.dirname(os.getcwd()), 'xlsx_data', f\"{'xlsx_#10_final'}.xlsx\")\n",
    "cname = os.path.join(os.path.dirname(os.getcwd()), 'xlsx_data', f\"{'xlsx_#10_case'}.xlsx\")\n",
    "lname = os.path.join(os.path.dirname(os.getcwd()), 'xlsx_data', f\"{'xlsx_#10_law'}.xlsx\")\n",
    "\n",
    "dff = pd.read_excel(fname, index_col=0)\n",
    "dfc = pd.read_excel(cname, index_col=0)\n",
    "dfl = pd.read_excel(lname, index_col=0)\n",
    "\n",
    "df = pd.concat([dff[['text', 'y_sentence']], dfc[1:], dfl[1:]], axis=1).reindex(dff.index)\n",
    "df = df.dropna(subset=['y_sentence'])\n",
    "df = df[df['y_sentence'] < 481] #살인 및 무기징역 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('y_sentence', axis=1)\n",
    "y = df['y_sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train = X_train[dfc.columns[1:]]\n",
    "X1_test = X_test[dfc.columns[1:]]\n",
    "\n",
    "X2_train = X_train[dfl.columns[1:]]\n",
    "X2_test = X_test[dfl.columns[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3_train = list(X_train[\"text\"])\n",
    "X3_test = list(X_test[\"text\"])\n",
    "\n",
    "max_features = 30000\n",
    "sequence_length = 256\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_features, split=' ', oov_token='<unw>')\n",
    "tokenizer.fit_on_texts(X3_train)\n",
    "\n",
    "X3_train = tokenizer.texts_to_sequences(X3_train)\n",
    "X3_test = tokenizer.texts_to_sequences(X3_test)\n",
    "\n",
    "X3_train = tf.keras.preprocessing.sequence.pad_sequences(X3_train, sequence_length)\n",
    "X3_test = tf.keras.preprocessing.sequence.pad_sequences(X3_test, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = tf.keras.Input(dtype = tf.float32, shape = (len(dfc.columns[1:]),))\n",
    "input_2 = tf.keras.Input(dtype = tf.float32, shape = (len(dfl.columns[1:]),))\n",
    "\n",
    "dense_layer_1_1 = tf.keras.layers.Dense(units = 20, activation = tf.nn.relu)(input_1)\n",
    "dense_layer_1_2 = tf.keras.layers.Dense(units = 20, activation = tf.nn.relu)(dense_layer_1_1)\n",
    "dense_layer_1_3 = tf.keras.layers.Dense(units = 20, activation = tf.nn.relu)(dense_layer_1_2)\n",
    "dense_layer_1_4 = tf.keras.layers.Dense(units = 20, activation = tf.nn.relu)(dense_layer_1_3)\n",
    "dropout_1_5 = tf.keras.layers.Dropout(rate = 0.5)(dense_layer_1_4)\n",
    "\n",
    "\n",
    "dense_layer_2_1 = tf.keras.layers.Dense(units = 20, activation = tf.nn.relu)(input_2)\n",
    "dense_layer_2_2 = tf.keras.layers.Dense(units = 20, activation = tf.nn.relu)(dense_layer_2_1)\n",
    "dense_layer_2_3 = tf.keras.layers.Dense(units = 20, activation = tf.nn.relu)(dense_layer_2_2)\n",
    "dense_layer_2_4 = tf.keras.layers.Dense(units = 20, activation = tf.nn.relu)(dense_layer_2_3)\n",
    "dropout_2_5 = tf.keras.layers.Dropout(rate = 0.5)(dense_layer_2_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join(os.path.dirname(os.getcwd()), 'data_processing', 'ft_0831.txt'),  encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "num_words = min(max_features, len(word_index)) + 1\n",
    "print(num_words)\n",
    "embedding_dim = 200\n",
    "num_filters = 100\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i > max_features:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        embedding_matrix[i] = np.random.randn(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_3 = tf.keras.Input(dtype = tf.float32, shape = (sequence_length,))\n",
    "embedding_layer_3 = tf.keras.layers.Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "                            input_length=sequence_length,\n",
    "                            trainable=True)(input_3)\n",
    "\n",
    "reshape_3 = tf.keras.layers.Reshape((sequence_length, embedding_dim, 1))(embedding_layer_3)\n",
    "\n",
    "conv_0_3 = tf.keras.layers.Conv2D(num_filters, kernel_size=(3, embedding_dim), activation=tf.nn.relu, kernel_regularizer=tf.keras.regularizers.l2(3))(reshape_3)\n",
    "conv_1_3 = tf.keras.layers.Conv2D(num_filters, kernel_size=(4, embedding_dim), activation=tf.nn.relu, kernel_regularizer=tf.keras.regularizers.l2(3))(reshape_3)\n",
    "conv_2_3 = tf.keras.layers.Conv2D(num_filters, kernel_size=(5, embedding_dim), activation=tf.nn.relu, kernel_regularizer=tf.keras.regularizers.l2(3))(reshape_3)\n",
    "\n",
    "maxpool_0_3 = tf.keras.layers.MaxPool2D(pool_size=(sequence_length - 3 + 1, 1), strides=(1,1), padding='valid')(conv_0_3)\n",
    "maxpool_1_3 = tf.keras.layers.MaxPool2D(pool_size=(sequence_length - 4 + 1, 1), strides=(1,1), padding='valid')(conv_1_3)\n",
    "maxpool_2_3 = tf.keras.layers.MaxPool2D(pool_size=(sequence_length - 5 + 1, 1), strides=(1,1), padding='valid')(conv_2_3)\n",
    "\n",
    "concatenated_tensor_3 = tf.keras.layers.Concatenate(axis=1)([maxpool_0_3, maxpool_1_3, maxpool_2_3])\n",
    "flatten_3 = tf.keras.layers.Flatten()(concatenated_tensor_3)\n",
    "dropout_3 = tf.keras.layers.Dropout(rate = 0.5)(flatten_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_layer = tf.keras.layers.Concatenate()([dropout_1_5, dropout_2_5, dropout_3])\n",
    "\n",
    "\n",
    "dense_layer_3 = tf.keras.layers.Dense(units = 20, activation = tf.nn.relu)(concat_layer)\n",
    "dense_layer_4 = tf.keras.layers.Dense(units = 20, activation = tf.nn.relu)(dense_layer_3)\n",
    "dense_layer_5 = tf.keras.layers.Dense(units = 20, activation = tf.nn.relu)(dense_layer_4)\n",
    "dense_layer_6 = tf.keras.layers.Dense(units = 20, activation = tf.nn.relu)(dense_layer_5)\n",
    "\n",
    "output = tf.keras.layers.Dense(units = 1, activation = tf.nn.relu)(dense_layer_6)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_1, input_2, input_3], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=tf.keras.optimizers.Adamax(learning_rate=0.005), metrics=['mse', 'mae'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, to_file='01_sentence.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "history = model.fit(x=[X1_train, X2_train, X3_train], y=y_train, batch_size=32, epochs=1000, verbose=1, validation_split=0.1, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['mse'])\n",
    "plt.plot(history.history['val_mse'])\n",
    "\n",
    "plt.title('model mse')\n",
    "plt.ylabel('mse')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x=[X1_test, X2_test, X3_test], y=y_test, verbose=1)\n",
    "\n",
    "print(score)\n",
    "#print(\"Test Score:\", score[0])\n",
    "print(\"Test MSE:\", score[1])\n",
    "print(\"Test MAE: \", score[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('01_sentence.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
