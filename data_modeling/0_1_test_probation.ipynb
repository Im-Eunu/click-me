{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "ipython = get_ipython()\n",
    "\n",
    "def hide_traceback(exc_tuple=None, filename=None, tb_offset=None,\n",
    "                      exception_only=False, running_compiled_code=False):\n",
    "       etype, value, tb = sys.exc_info()\n",
    "       return ipython._showtraceback(etype, value, ipython.InteractiveTB.get_exception_only(etype, value))\n",
    "\n",
    "ipython.showtraceback = hide_traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense, Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Input\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import regularizers\n",
    "from keras.initializers import Constant\n",
    "from keras.layers import *\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = os.path.join(os.path.dirname(os.getcwd()), 'xlsx_data', f\"{'xlsx_#10_final'}.xlsx\")\n",
    "cname = os.path.join(os.path.dirname(os.getcwd()), 'xlsx_data', f\"{'xlsx_#10_case'}.xlsx\")\n",
    "lname = os.path.join(os.path.dirname(os.getcwd()), 'xlsx_data', f\"{'xlsx_#10_law'}.xlsx\")\n",
    "\n",
    "dff = pd.read_excel(fname, index_col=0)\n",
    "dfc = pd.read_excel(cname, index_col=0)\n",
    "dfl = pd.read_excel(lname, index_col=0)\n",
    "\n",
    "df = pd.concat([dff[['text', 'y_probation']], dfc[1:], dfl[1:]], axis=1).reindex(dff.index)\n",
    "df = df.dropna(subset=['y_probation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = df[df['y_probation'] == 0].sample(frac=1)\n",
    "df_1 = df[df['y_probation'] == 1].sample(frac=1)\n",
    "\n",
    "sample_size = 1042\n",
    "\n",
    "df = pd.concat([df_0.head(sample_size), df_1.head(sample_size)]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('y_probation', axis=1)\n",
    "y = to_categorical(df['y_probation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train = X_train[dfc.columns[1:]]\n",
    "X1_test = X_test[dfc.columns[1:]]\n",
    "\n",
    "X2_train = X_train[dfl.columns[1:]]\n",
    "X2_test = X_test[dfl.columns[1:]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3_train = list(X_train[\"text\"])\n",
    "X3_test = list(X_test[\"text\"])\n",
    "\n",
    "max_features = 30000\n",
    "sequence_length = 128\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ', oov_token='<unw>')\n",
    "tokenizer.fit_on_texts(X3_train)\n",
    "\n",
    "X3_train = tokenizer.texts_to_sequences(X3_train)\n",
    "X3_test = tokenizer.texts_to_sequences(X3_test)\n",
    "\n",
    "X3_train = pad_sequences(X3_train, sequence_length)\n",
    "X3_test = pad_sequences(X3_test, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = Input(shape=(687,))\n",
    "input_2 = Input(shape=(662,))\n",
    "\n",
    "#dense_layer_1_0 = Dense(48, activation='relu')(input_1)\n",
    "dense_layer_1_1 = Dense(50, activation='relu')(input_1)\n",
    "dense_layer_1_2 = Dense(10, activation='relu')(dense_layer_1_1)\n",
    "\n",
    "dense_layer_1_3 = Dense(10, activation='relu')(dense_layer_1_2)\n",
    "\n",
    "dense_layer_1_4 = Dense(10, activation='relu')(dense_layer_1_3)\n",
    "\n",
    "dropout_1_5 = Dropout(0.2)(dense_layer_1_4)\n",
    "\n",
    "\n",
    "dense_layer_2_1 = Dense(50, activation='relu')(input_2)\n",
    "dense_layer_2_2 = Dense(10, activation='relu')(dense_layer_2_1)\n",
    "\n",
    "dense_layer_2_3 = Dense(10, activation='relu')(dense_layer_2_2)\n",
    "\n",
    "dense_layer_2_4 = Dense(10, activation='relu')(dense_layer_2_3)\n",
    "\n",
    "dropout_2_5 = Dropout(0.2)(dense_layer_2_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17584\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join(os.path.dirname(os.getcwd()), 'data_processing', 'ft_0814t.txt'),  encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "num_words = min(max_features, len(word_index)) + 1\n",
    "print(num_words)\n",
    "embedding_dim = 128\n",
    "num_filters = 100\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i > max_features:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        embedding_matrix[i] = np.random.randn(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_3 = Input(shape=(sequence_length,), dtype='int32')\n",
    "embedding_layer_3 = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=sequence_length,\n",
    "                            trainable=True)(input_3)\n",
    "\n",
    "reshape_3 = Reshape((sequence_length, embedding_dim, 1))(embedding_layer_3)\n",
    "\n",
    "conv_0_3 = Conv2D(num_filters, kernel_size=(3, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_3)\n",
    "conv_1_3 = Conv2D(num_filters, kernel_size=(4, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_3)\n",
    "conv_2_3 = Conv2D(num_filters, kernel_size=(5, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_3)\n",
    "conv_3_3 = Conv2D(num_filters, kernel_size=(6, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_3)\n",
    "\n",
    "maxpool_0_3 = MaxPool2D(pool_size=(sequence_length - 3 + 1, 1), strides=(1,1), padding='valid')(conv_0_3)\n",
    "maxpool_1_3 = MaxPool2D(pool_size=(sequence_length - 4 + 1, 1), strides=(1,1), padding='valid')(conv_1_3)\n",
    "maxpool_2_3 = MaxPool2D(pool_size=(sequence_length - 5 + 1, 1), strides=(1,1), padding='valid')(conv_2_3)\n",
    "maxpool_3_3 = MaxPool2D(pool_size=(sequence_length - 6 + 1, 1), strides=(1,1), padding='valid')(conv_3_3)\n",
    "\n",
    "concatenated_tensor_3 = Concatenate(axis=1)([maxpool_0_3, maxpool_1_3, maxpool_2_3, maxpool_3_3])\n",
    "flatten_3 = Flatten()(concatenated_tensor_3)\n",
    "dropout_3 = Dropout(0.2)(flatten_3)\n",
    "#dense_layer_3 = Dense(12, activation='relu')(dropout_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_layer = Concatenate()([dropout_1_5, dropout_2_5, dropout_3])\n",
    "\n",
    "dense_layer_3 = Dense(50, activation='relu')(concat_layer)\n",
    "\n",
    "dense_layer_4 = Dense(10, activation='relu')(dense_layer_3)\n",
    "dense_layer_5 = Dense(10, activation='relu')(dense_layer_4)\n",
    "dense_layer_6 = Dense(10, activation='relu')(dense_layer_5)\n",
    "\n",
    "output = Dense(2, activation='softmax')(dense_layer_6)\n",
    "\n",
    "model = Model(inputs=[input_1, input_2, input_3], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 128, 128)     2250752     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 687)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 662)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 128, 128, 1)  0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 50)           34400       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 50)           33150       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 126, 1, 100)  38500       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 125, 1, 100)  51300       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 124, 1, 100)  64100       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 123, 1, 100)  76900       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10)           510         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 10)           510         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 10)           110         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 10)           110         dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 4, 1, 100)    0           max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "                                                                 max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 10)           110         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 10)           110         dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 400)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 10)           0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 10)           0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 400)          0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 420)          0           dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 50)           21050       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 10)           510         dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 10)           110         dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 10)           110         dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 2)            22          dense_12[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,572,364\n",
      "Trainable params: 2,572,364\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1687 samples, validate on 188 samples\n",
      "Epoch 1/50\n",
      "1687/1687 [==============================] - 3s 2ms/step - loss: 4.5556 - acc: 0.4914 - val_loss: 0.7272 - val_acc: 0.4840\n",
      "Epoch 2/50\n",
      "1687/1687 [==============================] - 1s 335us/step - loss: 0.7021 - acc: 0.5282 - val_loss: 0.7011 - val_acc: 0.5638\n",
      "Epoch 3/50\n",
      "1687/1687 [==============================] - 1s 346us/step - loss: 0.7033 - acc: 0.6129 - val_loss: 0.7080 - val_acc: 0.5426\n",
      "Epoch 4/50\n",
      "1687/1687 [==============================] - 1s 327us/step - loss: 0.7073 - acc: 0.6509 - val_loss: 0.7090 - val_acc: 0.6330\n",
      "Epoch 5/50\n",
      "1687/1687 [==============================] - 1s 331us/step - loss: 0.7097 - acc: 0.6627 - val_loss: 0.7115 - val_acc: 0.6543\n",
      "Epoch 6/50\n",
      "1687/1687 [==============================] - 1s 327us/step - loss: 0.7113 - acc: 0.6846 - val_loss: 0.7118 - val_acc: 0.7181\n",
      "Epoch 7/50\n",
      "1687/1687 [==============================] - 1s 344us/step - loss: 0.7134 - acc: 0.7024 - val_loss: 0.7127 - val_acc: 0.7340\n",
      "Epoch 8/50\n",
      "1687/1687 [==============================] - 1s 327us/step - loss: 0.7146 - acc: 0.6882 - val_loss: 0.7157 - val_acc: 0.7447\n",
      "Epoch 9/50\n",
      "1687/1687 [==============================] - 1s 329us/step - loss: 0.7166 - acc: 0.7036 - val_loss: 0.7205 - val_acc: 0.6649\n",
      "Epoch 10/50\n",
      "1687/1687 [==============================] - 1s 328us/step - loss: 0.7193 - acc: 0.7090 - val_loss: 0.7239 - val_acc: 0.6915\n",
      "Epoch 11/50\n",
      "1687/1687 [==============================] - 1s 327us/step - loss: 0.7209 - acc: 0.7244 - val_loss: 0.7277 - val_acc: 0.6755\n",
      "Epoch 12/50\n",
      "1687/1687 [==============================] - 1s 331us/step - loss: 0.7213 - acc: 0.7350 - val_loss: 0.7300 - val_acc: 0.6596\n",
      "Epoch 13/50\n",
      "1687/1687 [==============================] - 1s 335us/step - loss: 0.7234 - acc: 0.7475 - val_loss: 0.7271 - val_acc: 0.7979\n",
      "Epoch 14/50\n",
      "1687/1687 [==============================] - 1s 332us/step - loss: 0.7218 - acc: 0.7534 - val_loss: 0.7228 - val_acc: 0.8245\n",
      "Epoch 15/50\n",
      "1687/1687 [==============================] - 1s 334us/step - loss: 0.7220 - acc: 0.7753 - val_loss: 0.7267 - val_acc: 0.7979\n",
      "Epoch 16/50\n",
      "1687/1687 [==============================] - 1s 336us/step - loss: 0.7202 - acc: 0.7860 - val_loss: 0.7232 - val_acc: 0.8351\n",
      "Epoch 17/50\n",
      "1687/1687 [==============================] - 1s 326us/step - loss: 0.7167 - acc: 0.7973 - val_loss: 0.7059 - val_acc: 0.8777\n",
      "Epoch 18/50\n",
      "1687/1687 [==============================] - 1s 332us/step - loss: 0.7037 - acc: 0.8198 - val_loss: 0.7049 - val_acc: 0.8670\n",
      "Epoch 19/50\n",
      "1687/1687 [==============================] - 1s 344us/step - loss: 0.6791 - acc: 0.8500 - val_loss: 0.6756 - val_acc: 0.8245\n",
      "Epoch 20/50\n",
      "1687/1687 [==============================] - 1s 328us/step - loss: 0.6389 - acc: 0.8731 - val_loss: 0.6261 - val_acc: 0.8723\n",
      "Epoch 21/50\n",
      "1687/1687 [==============================] - 1s 323us/step - loss: 0.5973 - acc: 0.8826 - val_loss: 0.5757 - val_acc: 0.8883\n",
      "Epoch 22/50\n",
      "1687/1687 [==============================] - 1s 331us/step - loss: 0.5669 - acc: 0.8880 - val_loss: 0.5610 - val_acc: 0.8670\n",
      "Epoch 23/50\n",
      "1687/1687 [==============================] - 1s 334us/step - loss: 0.5223 - acc: 0.9004 - val_loss: 0.5127 - val_acc: 0.9043\n",
      "Epoch 24/50\n",
      "1687/1687 [==============================] - 1s 328us/step - loss: 0.5001 - acc: 0.9069 - val_loss: 0.4982 - val_acc: 0.8936\n",
      "Epoch 25/50\n",
      "1687/1687 [==============================] - 1s 333us/step - loss: 0.4875 - acc: 0.9046 - val_loss: 0.5212 - val_acc: 0.9043\n",
      "Epoch 26/50\n",
      "  32/1687 [..............................] - ETA: 0s - loss: 0.4307 - acc: 0.9375"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=[X1_train, X2_train, X3_train], y=y_train, batch_size=32, epochs=50, \n",
    "                   verbose=1, validation_split=0.1)# , callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x=[X1_test, X2_test, X3_test], y=y_test, verbose=1)\n",
    "\n",
    "print(\"Test Score:\", score[0])\n",
    "print(\"Test ACC:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
